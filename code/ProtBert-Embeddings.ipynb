{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "module(\"load\", \"cuda/10.2\")\n",
    "module(\"load\",\"pytorch/gpu-cuda-10.2/1.8.0\")\n",
    "module(\"load\", \"tensorflow/gpu-cuda-10.2/2.3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 10:22:38.745928: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn as sk\n",
    "from torch.optim import Optimizer\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AdamW, BertTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjlrestrepol\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2021-11-26 10:23:21.976834: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/jlrestrepol/prot_bert_bfd_embeddings/runs/39tayt5m\" target=\"_blank\">distinctive-donkey-54</a></strong> to <a href=\"https://wandb.ai/jlrestrepol/prot_bert_bfd_embeddings\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/jlrestrepol/prot_bert_bfd_embeddings/runs/39tayt5m?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14a1f38c5450>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"prot_bert_bfd_embeddings\", entity=\"jlrestrepol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL=true\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_LOG_MODEL=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if device_name.type != 'cuda':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "global path\n",
    "path = {'data':\"./\", 'models':'./'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_set(charge = np.nan):\n",
    "    \"\"\"Function that outputs train and validation set for a given charge state\"\"\"\n",
    "    fig1 = pd.read_pickle(path['data']+'Fig1_powerlaw.pkl')#loads in raw training data\n",
    "    fig1 = fig1[fig1['Modified sequence'].str.find('(')==-1]#Unmodified seqs\n",
    "    fig1.loc[:,'Modified sequence'] = fig1['Modified sequence'].str.replace('_','')\n",
    "    fig1.loc[:,'Modified sequence'] = fig1['Modified sequence'].str.replace('', ' ')\n",
    "    fig1.loc[:,'Modified sequence'] = fig1['Modified sequence'].apply(lambda x : x[1:-1])\n",
    "    label_complete = (fig1['CCS'] - fig1['predicted_ccs']).values#residual\n",
    "    features = fig1[fig1['Charge'] == charge]['Modified sequence']#choose points with given charge, drop charge feature because of 2 heads\n",
    "    label = label_complete[fig1['Charge'] == charge]#choose appropiate residuals\n",
    "    x_train, x_val, y_train, y_val = model_selection.train_test_split(features, label, test_size = 0.1, random_state=42)#train/val set split\n",
    "    global label_scaler\n",
    "    label_scaler = StandardScaler()\n",
    "    label_scaler.fit(y_train.reshape(-1, 1))\n",
    "    y_train = label_scaler.transform(y_train.reshape(-1, 1))\n",
    "    y_val = label_scaler.transform(y_val.reshape(-1, 1))\n",
    "    print(f\"The Initial Mean Squared Error is: {sk.metrics.mean_squared_error(fig1['CCS'], fig1['predicted_ccs'])}\")#prints initial error\n",
    "    return x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Rostlab/prot_bert_bfd\"\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint, do_lower_case=False )\n",
    "model = BertModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Initial Mean Squared Error is: 384.9004049893965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/jrlopez/conda-envs/ccs/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2229: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_val_set(2)\n",
    "train_tokenized = tokenizer(x_train.values.tolist(), padding='max_length', return_tensors='pt', max_length = 42)\n",
    "val_tokenized = tokenizer(x_val.values.tolist(), padding='max_length', return_tensors='pt', max_length = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set(charge = np.nan):\n",
    "    \"\"\"Function that outputs train and validation set for a given charge state\"\"\"\n",
    "    fig4 = pd.read_pickle(path['data']+'Fig4_powerlaw.pkl')#loads in raw training data\n",
    "    fig4 = fig4[fig4['Modified_sequence'].str.find('(')==-1]#Unmodified seqs\n",
    "    fig4.loc[:,'Modified_sequence'] = fig4['Modified_sequence'].str.replace('_','')\n",
    "    fig4.loc[:,'Modified_sequence'] = fig4['Modified_sequence'].str.replace('', ' ')\n",
    "    fig4.loc[:,'Modified_sequence'] = fig4['Modified_sequence'].apply(lambda x : x[1:-1])\n",
    "    label_complete = (fig4['CCS'] - fig4['predicted_ccs']).values#residual\n",
    "    x_test = fig4[fig4['Charge'] == charge]['Modified_sequence']#choose points with given charge, drop charge feature because of 2 heads\n",
    "    y_test = label_complete[fig4['Charge'] == charge]#choose appropiate residuals\n",
    "    print(f\"The Initial Mean Squared Error is: {sk.metrics.mean_squared_error(fig4['CCS'], fig4['predicted_ccs'])}\")#prints initial error\n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Initial Mean Squared Error is: 462.8173100703641\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = test_set(charge = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = torch.tensor(y_test.reshape(-1,1), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized = tokenizer(x_test.values.tolist(), padding='max_length', return_tensors='pt', max_length = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Val set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    step = 100000\n",
    "    with torch.no_grad():\n",
    "        for n in range(1,2):\n",
    "            print(n)\n",
    "            model = BertModel.from_pretrained(checkpoint)\n",
    "            dictionary = {'input_ids': train_tokenized['input_ids'][n*step:(n+1)*step],\n",
    "                          'token_type_ids': train_tokenized['token_type_ids'][n*step:(n+1)*step],\n",
    "                          'attention_mask': train_tokenized['attention_mask'][n*step:(n+1)*step]}\n",
    "            train_embeddings = model(**dictionary)[0]\n",
    "            with open(f'./embeddings/train_{n}.pickle', 'wb') as handle:\n",
    "                pickle.dump(train_embeddings, handle, protocol = 4)\n",
    "        \n",
    "        \n",
    "        print((n+1)*step)\n",
    "        dictionary = {'input_ids': train_tokenized['input_ids'][(n+1)*step:],\n",
    "                      'token_type_ids': train_tokenized['token_type_ids'][(n+1)*step:],\n",
    "                      'attention_mask': train_tokenized['attention_mask'][(n+1)*step:]}\n",
    "        train_embeddings = model(**dictionary)[0]\n",
    "        with open(f'./embeddings/train_{n+1}.pickle', 'wb') as handle:\n",
    "                pickle.dump(train_embeddings, handle, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./embeddings/train_0.pickle', 'rb') as pickle_file:\n",
    "    part0 = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./embeddings/train_1.pickle', 'rb') as pickle_file:\n",
    "    part1 = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./embeddings/train_2.pickle', 'rb') as pickle_file:\n",
    "    part2 = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([288585, 42, 1024])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor = torch.cat((part0, part1, part2), 0)\n",
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./embeddings/train_complete.pickle', 'wb') as handle:\n",
    "        pickle.dump(train_tensor, handle, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train.flatten(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./embeddings/train_labels.pickle', 'wb') as handle:\n",
    "        pickle.dump(y_train, handle, protocol = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model = BertModel.from_pretrained(checkpoint)\n",
    "    val_embeddings = model(**val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./embeddings/val_complete.pickle', 'wb') as handle:\n",
    "        pickle.dump(val_embeddings, handle, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = torch.tensor(y_val.flatten(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./embeddings/val_labels.pickle', 'wb') as handle:\n",
    "        pickle.dump(y_val, handle, protocol = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model = BertModel.from_pretrained(checkpoint)\n",
    "    test_embeddings = model(**test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./embeddings/test_complete.pickle', 'wb') as handle:\n",
    "        pickle.dump(test_embeddings, handle, protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = torch.reshape(y_test, (-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./embeddings/test_labels.pickle', 'wb') as handle:\n",
    "        pickle.dump(y_test, handle, protocol = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load-in train, val and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../embeddings/train_complete.pickle', 'rb') as pickle_file:\n",
    "    x_train = pickle.load(pickle_file)\n",
    "    #x_train = torch.mean(x_train, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../embeddings/val_complete.pickle', 'rb') as pickle_file:\n",
    "    x_val = pickle.load(pickle_file)[0]\n",
    "    #x_val = torch.mean(x_val, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../embeddings/test_complete.pickle', 'rb') as pickle_file:\n",
    "    x_test = pickle.load(pickle_file)[0]\n",
    "    #x_test = torch.mean(x_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../embeddings/train_labels.pickle', 'rb') as pickle_file:\n",
    "    y_train = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../embeddings/val_labels.pickle', 'rb') as pickle_file:\n",
    "    y_val = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../embeddings/test_labels.pickle', 'rb') as pickle_file:\n",
    "    y_test = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.view(len(y_train), 1)\n",
    "y_val = y_val.view(len(y_val), 1)\n",
    "y_test = y_test.view(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kim CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KimCNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static):\n",
    "        super(KimCNN, self).__init__()\n",
    "\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        C = class_num\n",
    "        Co = kernel_num\n",
    "        Ks = kernel_sizes\n",
    "        \n",
    "        self.static = static\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, Co, (K, D)) for K in Ks])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(len(Ks) * Co, C)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.static:\n",
    "            x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        logit = self.fc1(x)  # (N, C)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_num = x_train.shape[1]\n",
    "embed_dim = x_train.shape[2]\n",
    "class_num = 1\n",
    "kernel_num = 2\n",
    "kernel_sizes = [2, 3, 4]\n",
    "dropout = 0.5\n",
    "static = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KimCNN(\n",
    "    embed_num=embed_num,\n",
    "    embed_dim=embed_dim,\n",
    "    class_num=class_num,\n",
    "    kernel_num=kernel_num,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    dropout=dropout,\n",
    "    static=static,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer and Trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data(x, y, batch_size):\n",
    "    i, batch = 0, 0\n",
    "    for batch, i in enumerate(range(0, len(x) - batch_size, batch_size), 1):\n",
    "        x_batch = x[i : i + batch_size]\n",
    "        y_batch = y[i : i + batch_size]\n",
    "        yield x_batch, y_batch, batch\n",
    "    if i + batch_size < len(x):\n",
    "        yield x[i + batch_size :], y[i + batch_size :], batch + 1\n",
    "    if batch == 0:\n",
    "        yield x, y, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1442.925"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0] / (10 *20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train loss: 0.63. Train Loss Scaled: 89.38 Validation loss: 0.52. Validation loss scaled: 73.36, Elapsed time: 46.47s.\n",
      "Epoch 2 Train loss: 0.63. Train Loss Scaled: 89.06 Validation loss: 0.52. Validation loss scaled: 73.92, Elapsed time: 46.53s.\n",
      "Epoch 3 Train loss: 0.62. Train Loss Scaled: 88.71 Validation loss: 0.52. Validation loss scaled: 73.63, Elapsed time: 46.59s.\n",
      "Epoch 4 Train loss: 0.62. Train Loss Scaled: 88.73 Validation loss: 0.51. Validation loss scaled: 73.27, Elapsed time: 46.62s.\n",
      "Epoch 5 Train loss: 0.62. Train Loss Scaled: 88.63 Validation loss: 0.52. Validation loss scaled: 73.90, Elapsed time: 46.57s.\n",
      "Epoch 6 Train loss: 0.62. Train Loss Scaled: 88.91 Validation loss: 0.51. Validation loss scaled: 73.22, Elapsed time: 46.84s.\n",
      "Epoch 7 Train loss: 0.62. Train Loss Scaled: 88.60 Validation loss: 0.52. Validation loss scaled: 74.48, Elapsed time: 46.59s.\n",
      "Epoch 8 Train loss: 0.62. Train Loss Scaled: 88.62 Validation loss: 0.52. Validation loss scaled: 73.47, Elapsed time: 46.64s.\n",
      "Epoch 9 Train loss: 0.62. Train Loss Scaled: 88.48 Validation loss: 0.52. Validation loss scaled: 73.82, Elapsed time: 46.64s.\n",
      "Epoch 10 Train loss: 0.62. Train Loss Scaled: 88.54 Validation loss: 0.51. Validation loss scaled: 72.84, Elapsed time: 46.71s.\n",
      "Epoch 11 Train loss: 0.62. Train Loss Scaled: 88.36 Validation loss: 0.52. Validation loss scaled: 74.56, Elapsed time: 46.67s.\n",
      "Epoch 12 Train loss: 0.62. Train Loss Scaled: 88.46 Validation loss: 0.51. Validation loss scaled: 73.32, Elapsed time: 46.60s.\n",
      "Epoch 13 Train loss: 0.62. Train Loss Scaled: 88.36 Validation loss: 0.52. Validation loss scaled: 74.48, Elapsed time: 46.63s.\n",
      "Epoch 14 Train loss: 0.62. Train Loss Scaled: 88.43 Validation loss: 0.52. Validation loss scaled: 73.76, Elapsed time: 46.64s.\n",
      "Epoch 15 Train loss: 0.62. Train Loss Scaled: 88.68 Validation loss: 0.52. Validation loss scaled: 73.98, Elapsed time: 46.62s.\n",
      "Epoch 16 Train loss: 0.62. Train Loss Scaled: 88.61 Validation loss: 0.52. Validation loss scaled: 73.57, Elapsed time: 46.57s.\n",
      "Epoch 17 Train loss: 0.62. Train Loss Scaled: 88.41 Validation loss: 0.52. Validation loss scaled: 74.60, Elapsed time: 47.01s.\n",
      "Epoch 18 Train loss: 0.62. Train Loss Scaled: 88.56 Validation loss: 0.51. Validation loss scaled: 73.23, Elapsed time: 47.14s.\n",
      "Epoch 19 Train loss: 0.62. Train Loss Scaled: 88.29 Validation loss: 0.52. Validation loss scaled: 73.47, Elapsed time: 46.39s.\n",
      "Epoch 20 Train loss: 0.62. Train Loss Scaled: 88.33 Validation loss: 0.51. Validation loss scaled: 72.67, Elapsed time: 46.24s.\n",
      "Epoch 21 Train loss: 0.62. Train Loss Scaled: 88.20 Validation loss: 0.52. Validation loss scaled: 73.87, Elapsed time: 46.51s.\n",
      "Epoch 22 Train loss: 0.62. Train Loss Scaled: 88.21 Validation loss: 0.52. Validation loss scaled: 73.66, Elapsed time: 47.26s.\n",
      "Epoch 23 Train loss: 0.62. Train Loss Scaled: 88.45 Validation loss: 0.52. Validation loss scaled: 73.58, Elapsed time: 46.72s.\n",
      "Epoch 24 Train loss: 0.62. Train Loss Scaled: 88.17 Validation loss: 0.51. Validation loss scaled: 73.14, Elapsed time: 46.65s.\n",
      "Epoch 25 Train loss: 0.62. Train Loss Scaled: 88.10 Validation loss: 0.52. Validation loss scaled: 73.54, Elapsed time: 46.62s.\n",
      "Epoch 26 Train loss: 0.62. Train Loss Scaled: 88.46 Validation loss: 0.51. Validation loss scaled: 72.75, Elapsed time: 46.49s.\n",
      "Epoch 27 Train loss: 0.62. Train Loss Scaled: 88.39 Validation loss: 0.51. Validation loss scaled: 73.14, Elapsed time: 46.36s.\n",
      "Epoch 28 Train loss: 0.62. Train Loss Scaled: 88.28 Validation loss: 0.51. Validation loss scaled: 73.04, Elapsed time: 46.59s.\n",
      "Epoch 29 Train loss: 0.62. Train Loss Scaled: 88.32 Validation loss: 0.53. Validation loss scaled: 75.34, Elapsed time: 46.30s.\n",
      "Epoch 30 Train loss: 0.62. Train Loss Scaled: 88.11 Validation loss: 0.52. Validation loss scaled: 73.40, Elapsed time: 46.52s.\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_losses_scaled, val_losses_scaled = [], [], [], []\n",
    "\n",
    "wandb.watch(model)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "    train_loss_scaled = 0\n",
    "    val_loss_scaled = 0\n",
    "\n",
    "    model.train(True)\n",
    "    batch_idx = 0\n",
    "    for x_batch, y_batch, batch in generate_batch_data(x_train, y_train, batch_size):\n",
    "        y_pred = model(x_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        preds_scaled = label_scaler.inverse_transform(y_pred.detach().numpy().reshape(-1, 1))\n",
    "        labels_scaled = label_scaler.inverse_transform(y_batch.detach().numpy().reshape(-1, 1))\n",
    "        mse_scaled = mean_squared_error(preds_scaled, labels_scaled)\n",
    "        train_loss_scaled += mse_scaled.item()\n",
    "        \n",
    "    train_loss /= batch\n",
    "    train_losses.append(train_loss)\n",
    "    train_loss_scaled /= batch\n",
    "    train_losses_scaled.append(train_loss_scaled)\n",
    "    wandb.log({\"loss_train\": train_loss, \"loss_train_scaled\":train_loss_scaled})\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    model.eval() # disable dropout for deterministic output\n",
    "    with torch.no_grad(): # deactivate autograd engine to reduce memory usage and speed up computations\n",
    "        val_loss, batch = 0, 1\n",
    "        for x_batch, y_batch, batch in generate_batch_data(x_val, y_val, batch_size):\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds_scaled = label_scaler.inverse_transform(y_pred.detach().numpy().reshape(-1, 1))\n",
    "            labels_scaled = label_scaler.inverse_transform(y_batch.detach().numpy().reshape(-1, 1))\n",
    "            mse_scaled = mean_squared_error(preds_scaled, labels_scaled)\n",
    "            val_loss_scaled += mse_scaled.item() \n",
    "        \n",
    "        val_loss /= batch\n",
    "        val_losses.append(val_loss)\n",
    "        val_loss_scaled /= batch\n",
    "        val_losses_scaled.append(val_loss_scaled)\n",
    "        wandb.log({\"loss_val\": val_loss, \"loss_val_scaled\":val_loss_scaled})\n",
    "        \n",
    "    print(\n",
    "        \"Epoch %d Train loss: %.2f. Train Loss Scaled: %.2f Validation loss: %.2f. Validation loss scaled: %.2f, Elapsed time: %.2fs.\"\n",
    "        % (epoch + 1, train_losses[-1], train_losses_scaled[-1], val_losses[-1], val_losses_scaled[-1], elapsed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "charge = 2\n",
    "fig1 = pd.read_pickle(path['data']+'Fig1_powerlaw.pkl')#loads in raw training data\n",
    "fig1 = fig1[fig1['Modified sequence'].str.find('(')==-1]#Unmodified seqs\n",
    "fig1.loc[:,'Modified sequence'] = fig1['Modified sequence'].str.replace('_','')\n",
    "fig1.loc[:,'Modified sequence'] = fig1['Modified sequence'].str.replace('', ' ')\n",
    "fig1.loc[:,'Modified sequence'] = fig1['Modified sequence'].apply(lambda x : x[1:-1])\n",
    "label_complete = (fig1['CCS'] - fig1['predicted_ccs']).values#residual\n",
    "features = fig1[fig1['Charge'] == charge]['Modified sequence']#choose points with given charge, drop charge feature because of 2 heads\n",
    "label = label_complete[fig1['Charge'] == charge]#choose appropiate residuals\n",
    "x_train_np, x_test_np, y_train_np, y_test_np = model_selection.train_test_split(features, label, test_size = 0.1, random_state=42)#train/val set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ccs = y_pred.detach().numpy().flatten() + fig1.loc[x_test_np.index]['predicted_ccs'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs = fig1.loc[x_test_np.index]['CCS'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist((pred_ccs-ccs)/pred_ccs, bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyImageCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization, MaxPooling2D, Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-19 17:48:46.718959: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-19 17:48:46.720806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:31:00.0 name: A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.305GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-11-19 17:48:46.720833: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-11-19 17:48:46.795919: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-19 17:48:46.816870: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-19 17:48:46.836012: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-19 17:48:46.932399: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-19 17:48:46.978050: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-19 17:48:46.990964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-11-19 17:48:46.994178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2021-11-19 17:48:46.994203: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-11-19 17:55:01.115303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-19 17:55:01.115328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2021-11-19 17:55:01.115335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2021-11-19 17:55:01.120652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 37599 MB memory) -> physical GPU (device: 0, name: A100-SXM4-40GB, pci bus id: 0000:31:00.0, compute capability: 8.0)\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(width, height, depth, filters=(2, 3, 4), regress=False):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "        # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "    # loop over the number of filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # if this is the first CONV layer then set the input\n",
    "        # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "        # CONV => RELU => BN => POOL\n",
    "        x = Conv2D(f, (3, 1024), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(32)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    # apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = Dense(4)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        x = Dense(1, activation=\"linear\")(x)\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "model = create_cnn(x_train.shape[2], x_train.shape[1], 1, regress=True)\n",
    "opt = Adam(lr=5e-3)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/30\n",
      "   2/4510 [..............................] - ETA: 9:13 - loss: 2.9959WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0559s vs `on_train_batch_end` time: 0.1895s). Check your callbacks.\n",
      "4510/4510 [==============================] - 1174s 260ms/step - loss: 0.6350 - val_loss: 0.5522\n",
      "Epoch 2/30\n",
      "4510/4510 [==============================] - 1157s 257ms/step - loss: 0.5519 - val_loss: 0.5080\n",
      "Epoch 3/30\n",
      "4510/4510 [==============================] - 1156s 256ms/step - loss: 0.5254 - val_loss: 0.5125\n",
      "Epoch 4/30\n",
      "4510/4510 [==============================] - 1157s 256ms/step - loss: 0.5030 - val_loss: 0.5456\n",
      "Epoch 5/30\n",
      "4510/4510 [==============================] - 1157s 257ms/step - loss: 0.4862 - val_loss: 0.5483\n",
      "Epoch 6/30\n",
      "4510/4510 [==============================] - 1156s 256ms/step - loss: 0.4721 - val_loss: 0.5497\n",
      "Epoch 7/30\n",
      "4510/4510 [==============================] - 1157s 256ms/step - loss: 0.4621 - val_loss: 0.5191\n",
      "Epoch 8/30\n",
      "4510/4510 [==============================] - 1159s 257ms/step - loss: 0.4510 - val_loss: 0.5562\n",
      "Epoch 9/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.4452 - val_loss: 0.5108\n",
      "Epoch 10/30\n",
      "4510/4510 [==============================] - 1159s 257ms/step - loss: 0.4382 - val_loss: 0.6260\n",
      "Epoch 11/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.4323 - val_loss: 0.5574\n",
      "Epoch 12/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.4286 - val_loss: 0.5509\n",
      "Epoch 13/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.4231 - val_loss: 0.5274\n",
      "Epoch 14/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.4174 - val_loss: 0.5494\n",
      "Epoch 15/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.4138 - val_loss: 0.5161\n",
      "Epoch 16/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.4094 - val_loss: 0.6037\n",
      "Epoch 17/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.4067 - val_loss: 0.6035\n",
      "Epoch 18/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.4031 - val_loss: 0.5263\n",
      "Epoch 19/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.3996 - val_loss: 0.6169\n",
      "Epoch 20/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.3972 - val_loss: 0.5760\n",
      "Epoch 21/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.3956 - val_loss: 0.6156\n",
      "Epoch 22/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.3929 - val_loss: 0.6035\n",
      "Epoch 23/30\n",
      "4510/4510 [==============================] - 1158s 257ms/step - loss: 0.3908 - val_loss: 0.6045\n",
      "Epoch 24/30\n",
      "1120/4510 [======>.......................] - ETA: 14:08 - loss: 0.3842"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training model...\")\n",
    "history = model.fit(x=x_train.detach().numpy(), y=y_train.detach().numpy(), \n",
    "    validation_data=(x_val.detach().numpy(), y_val.detach().numpy()),\n",
    "    epochs=n_epochs, batch_size=64, verbose=1, callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=1e-3)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "history = model.fit(x=x_train.detach().numpy(), y=y_train.detach().numpy(), \n",
    "    validation_data=(x_val.detach().numpy(), y_val.detach().numpy()),\n",
    "    epochs=n_epochs, batch_size=64, verbose=1, callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "        'd_model': self.d_model,\n",
    "        'warmup_steps': self.warmup_steps,\n",
    "         }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 21:41:13.423001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:4b:00.0 name: A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.305GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-11-22 21:41:13.423081: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-11-22 21:41:13.423105: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-22 21:41:13.423120: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-22 21:41:13.423136: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-22 21:41:13.423150: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-22 21:41:13.423166: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-22 21:41:13.423181: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-11-22 21:41:13.424247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2021-11-22 21:41:13.424275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-22 21:41:13.424280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2021-11-22 21:41:13.424285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2021-11-22 21:41:13.425340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 37599 MB memory) -> physical GPU (device: 0, name: A100-SXM4-40GB, pci bus id: 0000:4b:00.0, compute capability: 8.0)\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-22 21:37:43.688042: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-22 21:37:43.745662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:4b:00.0 name: A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.305GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-11-22 21:37:43.745712: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-11-22 21:37:43.748146: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-22 21:37:43.750447: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-22 21:37:43.750928: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-22 21:37:43.753427: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-22 21:37:43.754452: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-22 21:37:43.754798: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-11-22 21:37:43.758000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2021-11-22 21:37:43.760650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:4b:00.0 name: A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.305GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2021-11-22 21:37:43.760681: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-11-22 21:37:43.760696: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-22 21:37:43.760707: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-22 21:37:43.760718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-22 21:37:43.760729: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-22 21:37:43.760740: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-22 21:37:43.760751: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-11-22 21:37:43.763804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2021-11-22 21:37:43.763831: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-11-22 21:38:59.167426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-22 21:38:59.167450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2021-11-22 21:38:59.167462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2021-11-22 21:38:59.172492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 37599 MB memory) -> physical GPU (device: 0, name: A100-SXM4-40GB, pci bus id: 0000:4b:00.0, compute capability: 8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadSelfAttention.call of <__main__.MultiHeadSelfAttention object at 0x1497b1814c50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method MultiHeadSelfAttention.call of <__main__.MultiHeadSelfAttention object at 0x1497b1814c50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 1024  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "#inputs = layers.Input(shape=(maxlen,))\n",
    "#embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "#x = embedding_layer(inputs)\n",
    "\n",
    "inputs = layers.Input(shape=(embed_dim,))\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(inputs)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "optimizer = tf.keras.optimizers.Adam(CustomSchedule(embed_dim), beta_1=0.9, beta_2=0.98, \n",
    "                                 epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_numpy = x_train.detach().numpy()\n",
    "x_val_numpy = x_val.detach().numpy()\n",
    "x_test_numpy = x_test.detach().numpy()\n",
    "y_train_numpy = y_train.detach().numpy()\n",
    "y_test_numpy = y_test.detach().numpy()\n",
    "y_val_numpy = y_val.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288585 Training data\n",
      "32066 Test data\n",
      "Epoch 1/30\n",
      "4510/4510 [==============================] - 22s 5ms/step - loss: 0.5943 - val_loss: 0.5348\n",
      "Epoch 2/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5920 - val_loss: 0.5326\n",
      "Epoch 3/30\n",
      "4506/4510 [============================>.] - ETA: 0s - loss: 0.5893"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Layer TransformerBlock has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5893 - val_loss: 0.5230\n",
      "Epoch 4/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5904 - val_loss: 0.5283\n",
      "Epoch 5/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5847 - val_loss: 0.5412\n",
      "Epoch 6/30\n",
      "4510/4510 [==============================] - 22s 5ms/step - loss: 0.5815 - val_loss: 0.5262\n",
      "Epoch 7/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5797 - val_loss: 0.5250\n",
      "Epoch 8/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5775 - val_loss: 0.5231\n",
      "Epoch 9/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5762 - val_loss: 0.5261\n",
      "Epoch 10/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5747 - val_loss: 0.5263\n",
      "Epoch 11/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5739 - val_loss: 0.5230\n",
      "Epoch 12/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5754 - val_loss: 0.5250\n",
      "Epoch 13/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5697 - val_loss: 0.5206\n",
      "Epoch 14/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5714 - val_loss: 0.5219\n",
      "Epoch 15/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5694 - val_loss: 0.5256\n",
      "Epoch 16/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5663 - val_loss: 0.5273\n",
      "Epoch 17/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5650 - val_loss: 0.5192\n",
      "Epoch 18/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5637 - val_loss: 0.5188\n",
      "Epoch 19/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5640 - val_loss: 0.5230\n",
      "Epoch 20/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5624 - val_loss: 0.5249\n",
      "Epoch 21/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5610 - val_loss: 0.5212\n",
      "Epoch 22/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5590 - val_loss: 0.5149\n",
      "Epoch 23/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5574 - val_loss: 0.5199\n",
      "Epoch 24/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5581 - val_loss: 0.5190\n",
      "Epoch 25/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5581 - val_loss: 0.5178\n",
      "Epoch 26/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5562 - val_loss: 0.5209\n",
      "Epoch 27/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5581 - val_loss: 0.5149\n",
      "Epoch 28/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5567 - val_loss: 0.5148\n",
      "Epoch 29/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5543 - val_loss: 0.5153\n",
      "Epoch 30/30\n",
      "4510/4510 [==============================] - 21s 5ms/step - loss: 0.5540 - val_loss: 0.5199\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_numpy), \"Training data\")\n",
    "print(len(x_val_numpy), \"Test data\")\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
    "history = model.fit(\n",
    "    x_train_numpy, y_train_numpy, batch_size=64, epochs=30, validation_data=(x_val_numpy, y_val_numpy), callbacks=[WandbCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ZMQDisplayPublisher' object has no attribute '_orig_publish'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_60301/2127913808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/conda-envs/ccs/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   2712\u001b[0m     \"\"\"\n\u001b[1;32m   2713\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-envs/ccs/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_teardown_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTeardownStage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEARLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m                 \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atexit_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-envs/ccs/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_jupyter_teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"_pause_backend\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post_run_cell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_pub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_pub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_publish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_pub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_publish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout, BatchNormalization\n",
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(x_train_numpy.shape[1],)))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='he_normal', kernel_constraint=max_norm(4.)))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_normal', kernel_constraint=max_norm(4.)))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu', kernel_constraint=max_norm(4.)))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu', kernel_constraint=max_norm(4.)))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(32, activation='relu', kernel_constraint=max_norm(4.)))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(16, activation='relu', kernel_constraint=max_norm(4.)))\n",
    "model.add(BatchNormalization(axis = -1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_60301/1152991605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx_train_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWandbCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m~/conda-envs/ccs/lib/python3.7/site-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, monitor, verbose, mode, save_weights_only, log_weights, log_gradients, save_model, training_data, validation_data, labels, data_type, predictions, generator, input_type, output_type, log_evaluation, validation_steps, class_colors, log_batch_frequency, log_best_prefix, save_graph, validation_indexes, validation_row_processor, prediction_row_processor, infer_missing_processors, log_evaluation_frequency)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model-best.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model-best.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-envs/ccs/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, glob_str, base_path, policy)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mfiles_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwandb_glob_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-envs/ccs/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_files\u001b[0;34m(self, files_dict)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-envs/ccs/lib/python3.7/site-packages/wandb/sdk/interface/interface_queue.py\u001b[0m in \u001b[0;36m_publish_files\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFilesRecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_communicate_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_artifact\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogArtifactRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda-envs/ccs/lib/python3.7/site-packages/wandb/sdk/interface/interface_queue.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=adam)\n",
    "history = model.fit(\n",
    "    x_train_numpy, y_train_numpy, batch_size=64, epochs=60, validation_data=(x_val_numpy, y_val_numpy), callbacks=[WandbCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_numpy = x_train.detach().numpy()\n",
    "x_val_numpy = x_val.detach().numpy()\n",
    "x_test_numpy = x_test.detach().numpy()\n",
    "y_train_numpy = y_train.detach().numpy().ravel()\n",
    "y_test_numpy = y_test.detach().numpy().ravel()\n",
    "y_val_numpy = y_val.detach().numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=18)]: Using backend LokyBackend with 18 concurrent workers.\n",
      "[Parallel(n_jobs=18)]: Done   2 out of  18 | elapsed:   15.6s remaining:  2.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.125906705856323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=18)]: Done  18 out of  18 | elapsed:   31.0s finished\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "n_estimators = 30\n",
    "regr = BaggingRegressor(base_estimator=LinearSVR(dual=True, loss='squared_epsilon_insensitive'), n_estimators=n_estimators, \n",
    "random_state=0, n_jobs=-1, max_samples= 1.0/n_estimators, verbose = 1)\n",
    "regr.fit(x_train_numpy, y_train_numpy)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./svr_ch2']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(regr, './svr_ch2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=18)]: Using backend LokyBackend with 18 concurrent workers.\n",
      "[Parallel(n_jobs=18)]: Done   2 out of  18 | elapsed:    0.6s remaining:    5.0s\n",
      "[Parallel(n_jobs=18)]: Done  18 out of  18 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "pred = regr.predict(x_val_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6011313741609197"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(pred, y_val_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgr = xgboost.XGBRegressor(colsample_bytree = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1.0, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=18, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgr.fit(x_train_numpy, y_train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(xgr, './xgb_ch2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgr.predict(x_val_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6213196"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(pred, y_val_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ccs)",
   "language": "python",
   "name": "ccs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
